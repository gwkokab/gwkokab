{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gradf/miniforge3/envs/gwkenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/gradf/miniforge3/envs/gwkenv/lib/python3.11/site-packages/ligo/lw/lsctables.py:89: UserWarning: Wswiglal-redir-stdio:\n",
      "\n",
      "SWIGLAL standard output/error redirection is enabled in IPython.\n",
      "This may lead to performance penalties. To disable locally, use:\n",
      "\n",
      "with lal.no_swig_redirect_standard_output_error():\n",
      "    ...\n",
      "\n",
      "To disable globally, use:\n",
      "\n",
      "lal.swig_redirect_standard_output_error(False)\n",
      "\n",
      "Note however that this will likely lead to error messages from\n",
      "LAL functions being either misdirected or lost when called from\n",
      "Jupyter notebooks.\n",
      "\n",
      "To suppress this warning, use:\n",
      "\n",
      "import warnings\n",
      "warnings.filterwarnings(\"ignore\", \"Wswiglal-redir-stdio\")\n",
      "import lal\n",
      "\n",
      "  import lal\n",
      "[Loading lalsimutils.py : MonteCarloMarginalization version]\n",
      "  scipy :  1.13.0\n",
      "  numpy :  1.26.4\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "from clu import metrics\n",
    "from flax import linen as nn\n",
    "from flax import struct\n",
    "from flax.training import train_state\n",
    "from jax import numpy as jnp\n",
    "from pprint import pprint\n",
    "from gwkokab.utils import get_key\n",
    "from absl import logging\n",
    "from flax import linen as nn\n",
    "from flax.metrics import tensorboard\n",
    "from flax.training import train_state\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import ml_collections\n",
    "import numpy as np\n",
    "import optax\n",
    "\n",
    "from jaxtyping import Array\n",
    "from typing import Any, Tuple, Union, Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_vt_file(file_path: str = \"./vt_1_200_1000.hdf5\") -> Sequence[Array]:\n",
    "    \"\"\"Interpolates the VT values from an HDF5 file based on given m1 and m2 coordinates.\n",
    "\n",
    "    :param m1: The m1 coordinate.\n",
    "    :param m2: The m2 coordinate.\n",
    "    :param file_path: The path to the HDF5 file, defaults to \"./vt_1_200_1000.hdf5\"\n",
    "    :return: The interpolated VT value.\n",
    "    \"\"\"\n",
    "    with h5py.File(file_path, \"r\") as hdf5_file:\n",
    "        m1_grid = hdf5_file[\"m1\"][:]\n",
    "        m2_grid = hdf5_file[\"m2\"][:]\n",
    "        VT_grid = hdf5_file[\"VT\"][:]\n",
    "        m1_coord = m1_grid[0]\n",
    "        m2_coord = m2_grid[:, 0]\n",
    "\n",
    "    return m1_coord, m2_coord, VT_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralVT(nn.Module):\n",
    "    \"\"\"A neural network that approximates the VT function.\n",
    "\n",
    "    Dense(2)->ReLU->Dense(128)->ReLU->Dense(128)->ReLU->Dense(1)\n",
    "    \"\"\"\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        x = args[0]\n",
    "        # x = nn.Dense(2)(x)\n",
    "        # x = nn.softmax(x)\n",
    "        x = nn.Dense(32)(x)\n",
    "        x = nn.softmax(x)\n",
    "        x = nn.Dense(32)(x)\n",
    "        x = nn.softmax(x)\n",
    "        x = nn.Dense(1)(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def apply_model(state, m1m2, vt):\n",
    "\n",
    "    def loss_fn(params):\n",
    "        vt_pred = state.apply_fn(params, m1m2)\n",
    "        return jnp.mean((vt - vt_pred) ** 2)\n",
    "\n",
    "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "    (loss,), grads = grad_fn(state.params)\n",
    "    accuracy = metrics.accuracy(vt, state.apply_fn(state.params, m1m2))\n",
    "    return grads, loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def update_model(state, grads):\n",
    "    return state.apply_gradients(grads=grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(state, train_ds, batch_size, rng):\n",
    "    train_ds_size = len(train_ds[\"m1m2\"])\n",
    "    steps_per_epoch = train_ds_size // batch_size\n",
    "\n",
    "    perms = jax.random.permutation(rng, len(train_ds[\"m1m2\"]))\n",
    "    perms = perms[: steps_per_epoch * batch_size]  # skip incomplete batch\n",
    "    perms = perms.reshape((steps_per_epoch, batch_size))\n",
    "\n",
    "    epoch_loss = []\n",
    "    epoch_accuracy = []\n",
    "\n",
    "    for perm in perms:\n",
    "        batch_images = train_ds[\"m1m2\"][perm, ...]\n",
    "        batch_labels = train_ds[\"label\"][perm, ...]\n",
    "        grads, loss, accuracy = apply_model(state, batch_images, batch_labels)\n",
    "        state = update_model(state, grads)\n",
    "        epoch_loss.append(loss)\n",
    "        epoch_accuracy.append(accuracy)\n",
    "    train_loss = jnp.mean(epoch_loss)\n",
    "    train_accuracy = jnp.mean(epoch_accuracy)\n",
    "    return state, train_loss, train_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets():\n",
    "    \"\"\"Load MNIST train and test datasets into memory.\"\"\"\n",
    "    m1_from_file, m2_from_file, VT_from_file = read_vt_file()\n",
    "    m1_mesh, m2_mesh = jnp.meshgrid(m1_from_file, m2_from_file)\n",
    "\n",
    "    m1 = m1_mesh.flatten()\n",
    "    m2 = m2_mesh.flatten()\n",
    "    VT = VT_from_file.flatten().reshape(-1, 1)\n",
    "\n",
    "    train_to_test_ratio = 0.7\n",
    "    train_size = int(len(m1) * train_to_test_ratio)\n",
    "\n",
    "    # randomly shuffle the data and split into train and test sets\n",
    "    key = get_key()\n",
    "    perm = jax.random.permutation(key, len(m1))\n",
    "    m1, m2, VT = m1[perm, ...], m2[perm, ...], VT[perm, ...]\n",
    "    m1_train, m2_train, VT_train = m1[:train_size], m2[:train_size], VT[:train_size]\n",
    "    m1_test, m2_test, VT_test = m1[train_size:], m2[train_size:], VT[train_size:]\n",
    "\n",
    "    m1m2_train = jnp.column_stack((m1_train, m2_train))\n",
    "    m1m2_test = jnp.column_stack((m1_test, m2_test))\n",
    "\n",
    "    return (\n",
    "        {\"m1m2\": m1m2_train, \"label\": VT_train},\n",
    "        {\"m1m2\": m1m2_test, \"label\": VT_test},\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_state(rng, config):\n",
    "    \"\"\"Creates initial `TrainState`.\"\"\"\n",
    "    nvt = NeuralVT()\n",
    "    params = nvt.init(rng, jnp.ones([1, 28, 28, 1]))[\"params\"]\n",
    "    tx = optax.sgd(learning_rate=1e-3, momentum=0.9)\n",
    "    return train_state.TrainState.create(\n",
    "        apply_fn=nvt.apply,\n",
    "        params=params,\n",
    "        tx=tx,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(\n",
    "    config: ml_collections.ConfigDict,\n",
    "    workdir: str,\n",
    ") -> train_state.TrainState:\n",
    "    \"\"\"Execute model training and evaluation loop.\n",
    "\n",
    "    Args:\n",
    "      config: Hyperparameter configuration for training and evaluation.\n",
    "      workdir: Directory where the tensorboard summaries are written to.\n",
    "\n",
    "    Returns:\n",
    "      The train state (which includes the `.params`).\n",
    "    \"\"\"\n",
    "    train_ds, test_ds = get_datasets()\n",
    "    rng = get_key()\n",
    "\n",
    "    summary_writer = tensorboard.SummaryWriter(workdir)\n",
    "    summary_writer.hparams(dict(config))\n",
    "\n",
    "    rng, init_rng = jax.random.split(rng)\n",
    "    state = create_train_state(init_rng, config)\n",
    "\n",
    "    for epoch in range(1, config.num_epochs + 1):\n",
    "        rng, input_rng = jax.random.split(rng)\n",
    "        state, train_loss, train_accuracy = train_epoch(state, train_ds, config.batch_size, input_rng)\n",
    "        _, test_loss, test_accuracy = apply_model(state, test_ds[\"image\"], test_ds[\"label\"])\n",
    "\n",
    "        logging.info(\n",
    "            \"epoch:% 3d, train_loss: %.4f, train_accuracy: %.2f, test_loss: %.4f,\"\n",
    "            \" test_accuracy: %.2f\"\n",
    "            % (\n",
    "                epoch,\n",
    "                train_loss,\n",
    "                train_accuracy * 100,\n",
    "                test_loss,\n",
    "                test_accuracy * 100,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        summary_writer.scalar(\"train_loss\", train_loss, epoch)\n",
    "        summary_writer.scalar(\"train_accuracy\", train_accuracy, epoch)\n",
    "        summary_writer.scalar(\"test_loss\", test_loss, epoch)\n",
    "        summary_writer.scalar(\"test_accuracy\", test_accuracy, epoch)\n",
    "\n",
    "    summary_writer.flush()\n",
    "    return state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gwkenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
